# General settings for Hugging Face libraries
HF_HUB_DISABLE_TELEMETRY=1
HF_HUB_DISABLE_IMPLICIT_TOKEN=1
HF_HUB_DISABLE_XET=1 # disable downloading via XET protocol, as clearing XET cache needs to be done manually

# Hugging Face token required to access gated models
HF_TOKEN=

# Optionally set custom cache directory to store models (if not set standard HF directory will be used, usually ~/.cache/huggingface)
# HF_HOME=

# Custom inference settings
# USE_CPU=True # If not set (or False), GPU inference will be used
# NUM_THREADS=10 # If not set, llama-cpp will automatically assign
# STRUCTURED_OUTPUT_KEY=mykey # If not set, 'result' will be used
# TEMPERATURE=0.1 # If not set, 0.2 will be used (llama-cpp default)
CONTEXT_LENGTH=512 # If not set, 0 will be used (uses context window defined by model)
